{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import torch\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'].str.split().str.len().plot(kind='hist', bins=50, range=(0, 500))\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Review Length')\n",
    "plt.title('Review Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum word count\n",
    "max_word_count = df['reviewText'].str.split().str.len().max()\n",
    "print(f\"Maximum word count: {max_word_count}\")\n",
    "\n",
    "# Check distribution at higher thresholds\n",
    "for threshold in [50, 100, 500, 1000, 2000, 3000]:\n",
    "    count = (df['reviewText'].str.split().str.len() > threshold).sum()\n",
    "    print(f\"Reviews with more than {threshold} words: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reviews with More than 100 words\n",
    "# Set word count threshold\n",
    "threshold = 100  \n",
    "\n",
    "# Original size before filtering\n",
    "original_size = len(df)\n",
    "\n",
    "# Create a boolean mask for reviews below the threshold\n",
    "mask = df['reviewText'].str.split().str.len() <= threshold\n",
    "\n",
    "# Apply the mask to filter the DataFrame\n",
    "df = df[mask]\n",
    "\n",
    "# Reset index after filtering\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the new size\n",
    "new_size = len(df)\n",
    "\n",
    "print(f\"Original data size: {original_size}\")\n",
    "print(f\"New data size: {new_size}\")\n",
    "print(f\"Total of {original_size - new_size} reviews were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex cleaning\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function that applies multiple preprocessing steps.\n",
    "    Args:\n",
    "        text (str): Input text to be cleaned\n",
    "    Returns:\n",
    "        str: Cleaned text with punctuation, URLs, hashtags, usernames removed\n",
    "    \"\"\"\n",
    "    # Compile regex patterns once for better performance\n",
    "    patterns = [\n",
    "        (r'http\\S+', ''),           # Remove URLs more comprehensively\n",
    "        (r'#\\w+', ''),               # Remove hashtags\n",
    "        (r'@\\w+', ''),               # Remove usernames\n",
    "        (r'[^\\w\\s]', ''),            # Remove punctuation\n",
    "        (r'\\s+', ' '),               # Replace multiple whitespaces with single space\n",
    "        (r'\\s\\.?\\s', ' ')            # Remove single characters between spaces\n",
    "    ]\n",
    "    \n",
    "    # Apply all patterns in sequence\n",
    "    for pattern, repl in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the filtered data\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Label column\n",
    "# Load a sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"tabularisai/robust-sentiment-analysis\")\n",
    "\n",
    "# Apply the pipeline to the reviewText column\n",
    "df['sentiment'] = df['reviewText'].apply(lambda x: sentiment_pipeline(x)[0][\"label\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Unnamed column\n",
    "df = df.loc[:, ~df.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=45)\n",
    "plt.title('Sentiment Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"Very Negative\": 0, \"Negative\": 0,\"Very Positive\": 1, \"Positive\": 1, \"Neutral\": 2 }\n",
    "df['label'] = df['sentiment'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().plot(kind='bar', color=['green', 'blue', 'red'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_text, temp_text, train_sentiment, temp_sentiment = train_test_split(\n",
    "    df[\"reviewText\"].tolist(), df[\"sentiment\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_text, test_text, val_sentiment, test_sentiment = train_test_split(\n",
    "    temp_text, temp_sentiment, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "output_dir = 'models/semtiment_model'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    \"\"\"Tokenizes a list of texts using the pretrained tokenizer.\"\"\"\n",
    "    return tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")  # returns BatchEncoding object\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "Custom PyTorch Dataset for sentiment classification.\n",
    "Args:\n",
    "    encodings (Dict): Tokenized input encodings\n",
    "    labels (List): Corresponding sentiment labels\n",
    "\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = SentimentDataset(tokenize_texts(train_text), train_sentiment)\n",
    "val_dataset = SentimentDataset(tokenize_texts(val_text), val_sentiment)\n",
    "test_dataset = SentimentDataset(tokenize_texts(test_text), test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model.\n",
    "    Args:\n",
    "        pred: Prediction results from Trainer\n",
    "    Returns:\n",
    "        Dict of evaluation metrics\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(labels, preds, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': class_report['macro avg']['precision'],\n",
    "        'recall': class_report['macro avg']['recall'],\n",
    "        'f1': class_report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "def train(\n",
    "    train_dataset: Dataset, \n",
    "    val_dataset: Dataset,\n",
    "    batch_size: int = 16,\n",
    "    num_train_epochs: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the sentiment analysis model.\n",
    "    Args:\n",
    "        train_dataset (Dataset): Training dataset\n",
    "        val_dataset (Dataset): Validation dataset\n",
    "        batch_size (int): Training batch size\n",
    "        num_train_epochs (int): Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        learning_rate = 2e-5,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test dataset.\n",
    "    Args:\n",
    "        test_dataset (Dataset): Test dataset\n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load the Model for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer):\n",
    "    \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model = train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
