{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import torch\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'].str.split().str.len().plot(kind='hist', bins=50, range=(0, 500))\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Review Length')\n",
    "plt.title('Review Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum word count\n",
    "max_word_count = df['reviewText'].str.split().str.len().max()\n",
    "print(f\"Maximum word count: {max_word_count}\")\n",
    "\n",
    "# Check distribution at higher thresholds\n",
    "for threshold in [50, 100, 500, 1000, 2000, 3000]:\n",
    "    count = (df['reviewText'].str.split().str.len() > threshold).sum()\n",
    "    print(f\"Reviews with more than {threshold} words: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reviews with More than 100 words\n",
    "# Set word count threshold\n",
    "threshold = 100  \n",
    "\n",
    "# Original size before filtering\n",
    "original_size = len(df)\n",
    "\n",
    "# Create a boolean mask for reviews below the threshold\n",
    "mask = df['reviewText'].str.split().str.len() <= threshold\n",
    "\n",
    "# Apply the mask to filter the DataFrame\n",
    "df = df[mask]\n",
    "\n",
    "# Reset index after filtering\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the new size\n",
    "new_size = len(df)\n",
    "\n",
    "print(f\"Original data size: {original_size}\")\n",
    "print(f\"New data size: {new_size}\")\n",
    "print(f\"Total of {original_size - new_size} reviews were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex cleaning\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function that applies multiple preprocessing steps.\n",
    "    Args:\n",
    "        text (str): Input text to be cleaned\n",
    "    Returns:\n",
    "        str: Cleaned text with punctuation, URLs, hashtags, usernames removed\n",
    "    \"\"\"\n",
    "    # Compile regex patterns once for better performance\n",
    "    patterns = [\n",
    "        (r'http\\S+', ''),           # Remove URLs more comprehensively\n",
    "        (r'#\\w+', ''),               # Remove hashtags\n",
    "        (r'@\\w+', ''),               # Remove usernames\n",
    "        (r'[^\\w\\s]', ''),            # Remove punctuation\n",
    "        (r'\\s+', ' '),               # Replace multiple whitespaces with single space\n",
    "        (r'\\s\\.?\\s', ' ')            # Remove single characters between spaces\n",
    "    ]\n",
    "    \n",
    "    # Apply all patterns in sequence\n",
    "    for pattern, repl in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the filtered data\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Label column\n",
    "# Load a sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"tabularisai/robust-sentiment-analysis\")\n",
    "\n",
    "# Apply the pipeline to the reviewText column\n",
    "df['sentiment'] = df['reviewText'].apply(lambda x: sentiment_pipeline(x)[0][\"label\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Unnamed column\n",
    "df = df.loc[:, ~df.columns.str.match('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=45)\n",
    "plt.title('Sentiment Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"Very Positive\": 1, \"Positive\": 1, \"Neutral\": 2, \"Very Negative\": 0, \"Negative\": 0}\n",
    "df['label'] = df['sentiment'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentimet'].value_counts().plot(kind='bar', color=['green', 'blue', 'red'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_text, temp_text, train_sentiment, temp_sentiment = train_test_split(\n",
    "    df[\"reviewText\"].tolist(), df[\"sentiment\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_text, test_text, val_sentiment, test_sentiment = train_test_split(\n",
    "    temp_text, temp_sentiment, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the data using DistilBERT tokenizer -> Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "output_dir = 'models/semtiment_model'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'AutoModelForSequenceClassification'**\n",
    "- **Library:** Framework-agnostic (works with both PyTorch and TensorFlow).\n",
    "- **Purpose:** This is a higher-level, task-specific class designed for sequence classification tasks (e.g., sentiment analysis, text classification). It automatically loads a pretrained model (like DistilBERT, BERT, etc.) with a classification head on top, tailored for predicting labels.\n",
    "- **Output:** It returns logits (raw prediction scores) for each class, which can be passed through a softmax function to get probabilities. It’s ready to use for classification tasks out of the box.\n",
    "- **Use Case:** Use this when you want a prebuilt model for sequence classification without needing to manually add a classification head. It’s ideal for fine-tuning on labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    \"\"\"Tokenizes a list of texts using the pretrained tokenizer.\"\"\"\n",
    "    return tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")  # returns BatchEncoding object\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "Custom PyTorch Dataset for sentiment classification.\n",
    "Args:\n",
    "    encodings (Dict): Tokenized input encodings\n",
    "    labels (List): Corresponding sentiment labels\n",
    "\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
    "        } | {\"labels\": torch.tensor(self.labels[idx])}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = SentimentDataset(tokenize_texts(train_text), train_sentiment)\n",
    "val_dataset = SentimentDataset(tokenize_texts(val_text), val_sentiment)\n",
    "test_dataset = SentimentDataset(tokenize_texts(test_text), test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizerDistilBertTokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def encode_reviews(reviews, max_length=128):\n",
    "    return tokenizerDistilBertTokenizer(\n",
    "        reviews,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize train and validation sets\n",
    "train_encodings = encode_reviews(train_text)\n",
    "val_encodings = encode_reviews(val_text)\n",
    "test_encodings = encode_reviews(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset1 = SentimentDataset(train_encodings, train_sentiment)\n",
    "val_dataset1 = SentimentDataset(val_encodings, val_sentiment)\n",
    "test_dataset1 = SentimentDataset(test_encodings, test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred):\n",
    "#     \"\"\"\n",
    "#     Compute evaluation metrics for the model.\n",
    "#     Args:\n",
    "#         pred: Prediction results from Trainer\n",
    "#     Returns:\n",
    "#         Dict of evaluation metrics\n",
    "#     \"\"\"\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "#     # Compute accuracy\n",
    "#     accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "#     # Generate classification report\n",
    "#     class_report = classification_report(labels, preds, output_dict=True)\n",
    "    \n",
    "#     return {\n",
    "#         'accuracy': accuracy,\n",
    "#         'precision': class_report['macro avg']['precision'],\n",
    "#         'recall': class_report['macro avg']['recall'],\n",
    "#         'f1': class_report['macro avg']['f1-score']\n",
    "#     }\n",
    "\n",
    "# def train(\n",
    "#     train_dataset: Dataset, \n",
    "#     val_dataset: Dataset,\n",
    "#     batch_size: int = 16,\n",
    "#     num_train_epochs: int = 3\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Train the sentiment analysis model.\n",
    "#     Args:\n",
    "#         train_dataset (Dataset): Training dataset\n",
    "#         val_dataset (Dataset): Validation dataset\n",
    "#         batch_size (int): Training batch size\n",
    "#         num_train_epochs (int): Number of training epochs\n",
    "    \n",
    "#     Returns:\n",
    "#         Trained model\n",
    "#     \"\"\"\n",
    "#     # Training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         warmup_steps=500,\n",
    "#         weight_decay=0.01,\n",
    "#         logging_dir=f'{output_dir}/logs',\n",
    "#         logging_steps=10,\n",
    "#         evaluation_strategy='epoch',\n",
    "#         save_strategy='epoch',\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model='accuracy'\n",
    "#     )\n",
    "    \n",
    "#     # Initialize Trainer\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "#     )\n",
    "    \n",
    "#     # Train the model\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     logger = logging.getLogger(__name__)\n",
    "#     logger.info(\"Starting model training...\")\n",
    "#     trainer.train()\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute comprehensive evaluation metrics for the model.\n",
    "    \n",
    "    Args:\n",
    "        pred: Prediction results from Trainer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Suppress future warnings\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    # Extract labels and predictions\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute individual metrics\n",
    "    try:\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision = precision_score(labels, preds, average='macro')\n",
    "        recall = recall_score(labels, preds, average='macro')\n",
    "        f1 = f1_score(labels, preds, average='macro')\n",
    "        \n",
    "        # Generate detailed classification report\n",
    "        class_report = classification_report(labels, preds, output_dict=True)\n",
    "        \n",
    "        # Log the metrics\n",
    "        logging.info(\"Model Evaluation Metrics:\")\n",
    "        logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logging.info(f\"Precision: {precision:.4f}\")\n",
    "        logging.info(f\"Recall: {recall:.4f}\")\n",
    "        logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'detailed_report': class_report\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing metrics: {e}\")\n",
    "        return {}\n",
    "\n",
    "def train_model(\n",
    "    train_dataset: Dataset, \n",
    "    val_dataset: Dataset, \n",
    "    model: Any,  # Allow flexibility in model type\n",
    "    output_dir: str = f'{output_dir}/results',\n",
    "    batch_size: int = 16, \n",
    "    num_train_epochs: int = 3,\n",
    "    learning_rate: float = 2e-5\n",
    ") -> Trainer:\n",
    "    \"\"\"\n",
    "    Train the sentiment analysis model with comprehensive configuration.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (Dataset): Training dataset\n",
    "        val_dataset (Dataset): Validation dataset\n",
    "        model (Any): Model to be trained\n",
    "        output_dir (str): Directory to save model outputs\n",
    "        batch_size (int): Training batch size\n",
    "        num_train_epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate for training\n",
    "    \n",
    "    Returns:\n",
    "        Trained Trainer instance\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Prepare training arguments with enhanced configurability\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Epoch and learning configurations\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        \n",
    "        # Batch size configurations\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        \n",
    "        # Optimization parameters\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Logging configurations\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        \n",
    "        # Evaluation and saving strategies\n",
    "        eval_strategy='epoch',  # Updated from deprecated 'evaluation_strategy'\n",
    "        save_strategy='epoch',\n",
    "        \n",
    "        # Model selection criteria\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',  # More robust metric than accuracy\n",
    "        \n",
    "        # Additional training optimizations\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    "        dataloader_num_workers=4,  # Parallel data loading\n",
    "        \n",
    "        # Prevent overwriting\n",
    "        overwrite_output_dir=False\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer with comprehensive configuration\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,  # Stop if no improvement for 3 epochs\n",
    "                early_stopping_threshold=0.01  # Minimum improvement required\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Log training start\n",
    "    logger.info(\"Starting model training...\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Additional logging of training results\n",
    "        logger.info(\"Training completed successfully.\")\n",
    "        logger.info(f\"Total training time: {train_result.metrics.get('train_runtime', 'N/A')}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(train_dataset=train_dataset1, val_dataset=val_dataset1, model=model, output_dir=output_dir, batch_size=16, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test dataset.\n",
    "    Args:\n",
    "        test_dataset (Dataset): Test dataset\n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Test Evaluation Results:\")\n",
    "    for metric, value in eval_results.items():\n",
    "        logger.info(f\"{metric}: {value}\")\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load the Model for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self):\n",
    "    \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(self.output_dir)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(train_dataset=train_dataset, val_dataset=val_dataset, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "trained_model = train_model(train_dataset=train_dataset1, val_dataset=val_dataset1, model=model, output_dir=output_dir, batch_size=16, num_train_epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = trained_model.predict(test_dataset)\n",
    "test_metrics = compute_metrics(test_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def compute_metrics(pred) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute comprehensive evaluation metrics for the model.\n",
    "    \n",
    "    Args:\n",
    "        pred: Prediction results from Trainer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Suppress future warnings\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    # Extract labels and predictions\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute individual metrics\n",
    "    try:\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision = precision_score(labels, preds, average='macro')\n",
    "        recall = recall_score(labels, preds, average='macro')\n",
    "        f1 = f1_score(labels, preds, average='macro')\n",
    "        \n",
    "        # Generate detailed classification report\n",
    "        class_report = classification_report(labels, preds, output_dict=True)\n",
    "        \n",
    "        # Log the metrics\n",
    "        logging.info(\"Model Evaluation Metrics:\")\n",
    "        logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logging.info(f\"Precision: {precision:.4f}\")\n",
    "        logging.info(f\"Recall: {recall:.4f}\")\n",
    "        logging.info(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'detailed_report': class_report\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing metrics: {e}\")\n",
    "        return {}\n",
    "\n",
    "def train_model(\n",
    "    train_dataset: Dataset, \n",
    "    val_dataset: Dataset, \n",
    "    model: Any,  # Allow flexibility in model type\n",
    "    output_dir: str = './results',\n",
    "    batch_size: int = 16, \n",
    "    num_train_epochs: int = 3,\n",
    "    learning_rate: float = 2e-5\n",
    ") -> Trainer:\n",
    "    \"\"\"\n",
    "    Train the sentiment analysis model with comprehensive configuration.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (Dataset): Training dataset\n",
    "        val_dataset (Dataset): Validation dataset\n",
    "        model (Any): Model to be trained\n",
    "        output_dir (str): Directory to save model outputs\n",
    "        batch_size (int): Training batch size\n",
    "        num_train_epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate for training\n",
    "    \n",
    "    Returns:\n",
    "        Trained Trainer instance\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Prepare training arguments with enhanced configurability\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Epoch and learning configurations\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        \n",
    "        # Batch size configurations\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        \n",
    "        # Optimization parameters\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Logging configurations\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        \n",
    "        # Evaluation and saving strategies\n",
    "        eval_strategy='epoch',  # Updated from deprecated 'evaluation_strategy'\n",
    "        save_strategy='epoch',\n",
    "        \n",
    "        # Model selection criteria\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',  # More robust metric than accuracy\n",
    "        \n",
    "        # Additional training optimizations\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    "        dataloader_num_workers=4,  # Parallel data loading\n",
    "        \n",
    "        # Prevent overwriting\n",
    "        overwrite_output_dir=False\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer with comprehensive configuration\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,  # Stop if no improvement for 3 epochs\n",
    "                early_stopping_threshold=0.01  # Minimum improvement required\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Log training start\n",
    "    logger.info(\"Starting model training...\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Additional logging of training results\n",
    "        logger.info(\"Training completed successfully.\")\n",
    "        logger.info(f\"Total training time: {train_result.metrics.get('train_runtime', 'N/A')}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the training function.\n",
    "    \n",
    "    Note: Replace with actual dataset preparation\n",
    "    \"\"\"\n",
    "    # Placeholder for dataset and model initialization\n",
    "    # You would typically have:\n",
    "    # 1. Prepared train_dataset\n",
    "    # 2. Prepared val_dataset\n",
    "    # 3. Initialized model\n",
    "    \n",
    "    try:\n",
    "        # Example training call (commented out as placeholders)\n",
    "        # trained_model = train_model(\n",
    "        #     train_dataset=train_dataset, \n",
    "        #     val_dataset=val_dataset,\n",
    "        #     model=your_model,\n",
    "        #     output_dir='./sentiment_model_results'\n",
    "        # )\n",
    "        pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training process failed: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
